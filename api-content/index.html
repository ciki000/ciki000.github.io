{"posts":[{"title":"知识整理：暗光增强","content":"","link":"https://ciki000.github.io/post/zhi-shi-zheng-li-an-guang-zeng-qiang/"},{"title":"知识整理：对抗攻击","content":"","link":"https://ciki000.github.io/post/zhi-shi-zheng-li-aaai2022-tian-chi-tiao-zhan-zhe-di-ba-qi/"},{"title":"知识整理：深度学习","content":"OptimizerSGDMomentumNAGAdaGradRMSPropAdamAdamW牛顿迭代NormalizationBatchNormalizationLayerNormalizationInstanceNormalizationGroupNormalizationCNN空洞卷积（DilatedConvolution）深度可分离卷积（DepthwiseSeparableConvolution）Optimizer定义待优化参数：www目标函数：f(w)f\\left(w\\right)f(w)学习率：α\\alphaα当前梯度：gt=∇f(wt)g_{t}=\\nablaf\\left(w_{t}\\right)gt​=∇f(wt​)SGDwt+1=wt−α⋅gtw_{t+1}=w_{t}-\\alpha\\cdotg_{t}wt+1​=wt​−α⋅gt​批量梯度下降（BGD）：对整个训练数据计算梯度随机梯度下降（SGD）：随机选取一个训练样本计算梯度小批量梯度下降（MBGD）：对BatchsizeBatchsizeBatchsize个训练样本计算梯度通常SGD指小批量梯度下降因为只利用了当前的梯度信息，没有引入动量等信息，SGD收敛速度会较慢，且容易困在局部最优点和鞍点，在沟壑的两边持续振荡（因为局部最优点和鞍点的梯度为0）。Momentum引入一阶动量来加速SGD的收敛并抑制振荡mt=β⋅mt−1+α⋅gtwt+1=wt−mtm_{t}=\\beta\\cdotm_{t-1}+\\alpha\\cdotg_{t}\\\\w_{t+1}=w_{t}-m_{t}mt​=β⋅mt−1​+α⋅gt​wt+1​=wt​−mt​β\\betaβ通常为0.9NAG预知之后的梯度来提前变化，近似下一步的参数来计算梯度。mt=β⋅mt−1+α⋅∇f(wt−β⋅mt−1)wt+1=wt−mtm_{t}=\\beta\\cdotm_{t-1}+\\alpha\\cdot\\nablaf\\left(w_{t}-\\beta\\cdotm_{t-1}\\right)\\\\w_{t+1}=w_{t}-m_{t}mt​=β⋅mt−1​+α⋅∇f(wt​−β⋅mt−1​)wt+1​=wt​−mt​NAG本质上是近似了目标函数的二阶导数，因此比单纯使用Momentum收敛更快。AdaGrad引入二阶动量来自适应学习率，对高频（经常更新的）参数做较小的更新，对低频（很少更新的）参数做较大的更新。通过二阶动量（该维度上，到目前为止所有梯度值的平方和）来度量历史更新频率。Vt=∑τ=1tgτ2wt+1=wt−αVt+ϵ⋅gtV_{t}=\\sum_{\\tau=1}^{t}g_{\\tau}^{2}\\\\w_{t+1}=w_{t}-\\frac{\\alpha}{\\sqrt{V_{t}+\\epsilon}}\\cdotg_{t}Vt​=τ=1∑t​gτ2​wt+1​=wt​−Vt​+ϵ​α​⋅gt​在分母上加一个小的平滑项ϵ\\epsilonϵ防止为0AdaGrad存在的问题：VtV_{t}Vt​是单调递增的，因此会导致学习率不断减小。RMSProp通过滑动平均来计算二阶动量，避免其不断累积导致学习率不断减少。Vt=β⋅Vt−1+(1−β)⋅gt2wt+1=wt−αVt+ϵ⋅gtV_{t}=\\beta\\cdotV_{t-1}+\\left(1-\\beta\\right)\\cdotg_{t}^{2}\\\\w_{t+1}=w_{t}-\\frac{\\alpha}{\\sqrt{V_{t}+\\epsilon}}\\cdotg_{t}Vt​=β⋅Vt−1​+(1−β)⋅gt2​wt+1​=wt​−Vt​+ϵ​α​⋅gt​Adam引入了一阶动量，相当于RMSProp+Momentum。mt=β1⋅mt−1+(1−β1)⋅gtVt=β2⋅Vt−1+(1−β2)⋅gt2m_{t}=\\beta_{1}\\cdotm_{t-1}+\\left(1-\\beta_{1}\\right)\\cdotg_{t}\\\\V_{t}=\\beta_{2}\\cdotV_{t-1}+\\left(1-\\beta_{2}\\right)\\cdotg_{t}^{2}\\\\mt​=β1​⋅mt−1​+(1−β1​)⋅gt​Vt​=β2​⋅Vt−1​+(1−β2​)⋅gt2​由于mtm_{t}mt​与VtV_{t}Vt​初始值为0，因此会向0偏置，所以做了偏差校正。m^t=mt1−β1tV^t=Vt1−β2t\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}\\\\\\hat{V}_{t}=\\frac{V_{t}}{1-\\beta_{2}^{t}}m^t​=1−β1t​mt​​V^t​=1−β2t​Vt​​最终更新参数wt+1=wt−αV^t+ϵ⋅m^tw_{t+1}=w_{t}-\\frac{\\alpha}{\\sqrt{\\hat{V}_{t}+\\epsilon}}\\cdot\\hat{m}_{t}wt+1​=wt​−V^t​+ϵ​α​⋅m^t​AdamWAdam+weightdecayweightdecay和L2L_{2}L2​正则化的区别：weight-decay：wt+1=(1−λ)wt−αgtw_{t+1}=\\left(1-\\lambda\\right)w_{t}-\\alphag_{t}wt+1​=(1−λ)wt​−αgt​L2L_{2}L2​正则化：freg(w)=f(w)+λ2∥w∥22f^{reg}\\left(w\\right)=f\\left(w\\right)+\\frac{\\lambda}{2}\\Vertw\\Vert_2^2freg(w)=f(w)+2λ​∥w∥22​在SGD中weightdecay和L2L_{2}L2​正则化是等价的，但是对于Adam这类自适应学习率的优化算法weightdecay和L2L_{2}L2​正则化是不等价的。牛顿迭代求使目标函数的一阶导为0的参数值。求当前参数xxx对于目标函数的切线，该切线和xxx轴的交点x^\\hat{x}x^，作为新的xxx。重复该过程，直到交点和函数的零点重合。收敛速度快，但是对目标函数有严格要求，需要能求二阶偏导。海森矩阵的逆计算复杂，计算量较大。Normalizationy=γ(x−μ(x)σ(x))+βy=\\gamma\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right)+\\betay=γ(σ(x)x−μ(x)​)+β通过Normalization，将越来越偏的特征图的分布修正为相对标准的分布，使激活函数的输入值处于函数相对敏感的区域，从而使梯度值变大，加快收敛速度。避免梯度消失。缩放参数γ\\gammaγ和平移参数β\\betaβ可以让特征有不同的分布。BatchNormalization对一个Batch内所有特征图的通道进行归一化。LayerNormalization对单个特征图进行归一化。InstanceNormalization对单个特征图的通道进行归一化。最初用于图像风格迁移，防止Normalization使各个通道的均值和方差产生变化从而影响到最终生成图像的风格。GroupNormalization对单个特征图的通道分组进行归一化。CNN空洞卷积（DilatedConvolution）深度可分离卷积（DepthwiseSeparableConvolution）","link":"https://ciki000.github.io/post/mian-shi-zhi-shi-zheng-li-shen-du-xue-xi/"},{"title":"知识整理：机器学习","content":"特征归一化作用评估指标P-R曲线ROC曲线Logistic回归决策树支持向量机特征归一化作用使不同量纲的特征处于同一数量级，减少方差大的特征的影响。使各个特征的参数更新速度相对一致，有助于加速收敛。评估指标P-R曲线ROC曲线Logistic回归Logistic回归与线性回归的相同点都使用了极大似然估计来对训练样本进行建模。都使用了梯度下降的方法来求解参数。决策树支持向量机支持向量（supportvector）：离分割超平面最近的点","link":"https://ciki000.github.io/post/zhi-shi-zheng-li-ji-qi-xue-xi/"},{"title":"论文阅读：《Exploring Sparsity in Image Super-Resolution for Efficient Inference》","content":"目前基于CNN的超分辨率（SR）方法平等地处理所有位置，将计算资源在空间中统一分配。然而低分辨率图像中缺失的细节主要存在于边缘和纹理区域，平滑区域所需的计算资源较少，因此现有的基于CNN的方法在平滑区域存在冗余计算，增加了计算成本，限制了其在移动设备上的应用。本文通过研究图像稀疏性来提高网络的计算效率。具体地说，我们提出了SparseMaskSR（SMSR）网络来学习稀疏掩膜，以减少冗余计算。在我们的SMSR中，空间掩膜学习并识别图像中的“重要”区域，而通道掩膜学习并标记那些“不重要”区域中的冗余通道。因此，冗余计算可以被精确定位并跳过，同时网络可以保持相当的性能。MyNote最近看了三篇根据图片不同区域重建难度不同的思路进行超分的论文，这是第二篇。SMSR是采用了学习稀疏掩膜来进行稀疏卷积的方式减少计算量。与以往的自适应推理技术相比，SMSR同时考虑了空间冗余和通道冗余，生成了空间掩膜和通道掩膜，相当于是使用了一个三维的稀疏掩膜。SMSR能较准确的定位冗余计算来进行剪枝，但是缺点是稀疏卷积的加速需要硬件的支持。对于平滑区域的处理是直接剪枝还是使用较轻量级的模型来处理，两者的优劣各是什么？如果网络需要使用argmax层产生类似one-hot的分布，在训练时可以使用Gumbelsoftmax层代替来计算梯度。Gumbelsoftmax有点类似蒸馏网络中的softmax，由一个逐渐降低的温度控制。空间掩膜和通道掩膜的Gumbelsoftmax层公式如下：原来vanillaconvolution就是普通卷积的意思，一开始看名字”香草卷积“还以为又是什么没学过的高大上卷积2333。PaperCodeContributions我们提出了SMSR网络，其能动态地跳过冗余计算，从而实现高效的图像SR。与现有的轻量级网络设计相比，我们探索了一种不同的方案，通过修剪冗余计算来提高计算效率。我们提出通过学习空间掩膜和通道掩膜来定位冗余计算，这两种掩膜共同作用于冗余计算的精确定位。实验结果表明，我们的SMSR达到了SOTA性能并有更好的计算效率。SMSR","link":"https://ciki000.github.io/post/lun-wen-yue-du-lesslessexploring-sparsity-in-image-super-resolution-for-efficient-inferencegreatergreater/"},{"title":"论文阅读：《ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic》","content":"我们的目标是在大图像（2K-8K）上加速超分辨率（SR）网络。在实际应用中，通常将大图像分解成小的子图像。基于此，我们发现不同图像区域的恢复困难不同，因此可以通过不同容量的网络进行处理。直观地说，平滑的区域比复杂的纹理更容易进行超分。利用这一性质，我们可以采用合适的SR网络对分解后的不同子图像进行处理。因此我们提出了一种新的方法——ClassSR，它将分类网络和SR网络结合在一个统一的框架中。首先通过分类模块根据恢复困难程度将子图像分类，然后利用SR模块对不同的类进行超分。分类模块是一个传统的分类网络，而SR模块是一个网络容器，包含一些SR网络及其简化版本。在此基础上，提出了一种新的基于Class-Loss和Average-Loss的分类方法。联合训练后，大部分子图像将通过较小的网络计算，大大降低了计算成本。实验表明，我们的ClassSR可以帮助现有的大多数方法（如FSRCNN、CARN、SRResNet、RCAN）在DIV8K数据集中节省高达50%的FLOPs。这个通用框架也可以应用于其他low-level视觉任务。MyNote最近看了三篇根据图片不同区域重建难度不同的思路进行超分的论文，这是第一篇。ClassSR是采用了非常简单直接的思路：把图片分割成大小相同的矩形子图像块然后送入分类模块根据复原难度进行分类，最后根据类别送入不同的超分网络。ClassSR显然比较容易部署且可以和一些其他加速方法结合使用，但是我认为也有一些缺陷，首先事先把图片分割成大小相同的矩形子图像块会减少网络的感受区域，丢失一些较远的信息，因此该方法比较适用于加速大图片的超分，对小图片的加速效果有限。另外我认为如果图片的纹理区域比较不规则，即大部分子图像都既有复杂纹理又有简单区域，那么加速效果会较差。ClassSR的分类模块采用了Class-Loss与Average-Loss两个损失函数。Class-Loss可以使得分类结果的概率分布中的最大概率尽可能大，即尽可能接近one-hot编码。Average-Loss用于防止所有图像块都被分配到最复杂的分支，其中使用概率和代替统计量来传播（但是我感觉其中有个小问题，就是默认了所有类别的数量是一样的）。公式如下：另外训练过程中结果并不是由单一SR分支计算得出，而是由所有SR分支加权求和（感觉这种多阶段的网络的损失函数必须得考虑到前面模块的计算误差对后面模块的影响然后进行全局的优化，不然如果把各个模块分开进行优化可能会导致前面模块的误差在后面模块的计算中不断放大）。公式如下：训练由多步完成，首先固定预训练的SR模型来训练分类模块，然后对全局网络进行fine-tune。感觉这种多阶段的网络的训练都是通过多次训练来完成的，每次固定一些模块然后训练其余的模块，最后再fine-tune。不然如果直接从头全部一起训练就会很难收敛。PaperCodeContributions我们提出了ClassSR，这是第一个在子图像层面上将分类和超分辨率结合在一起的SR管道。我们根据数据的特性来处理加速问题。它使ClassSR与其他加速网络不冲突。压缩到极限的网络仍然可以被ClassSR加速。我们提出了具有两种新的损失函数的分类方法。该算法根据子图像的恢复困难程度进行分类，子图像由特定的分支而不是预先确定的标签进行处理，因此也可以直接应用于其他low-level视觉任务。ClassSR","link":"https://ciki000.github.io/post/lun-wen-yue-du-lesslessclasssr-a-general-framework-to-accelerate-super-resolution-networks-by-data-characteristicgreatergreater/"},{"title":"论文阅读：《Attention in Attention Network for Image Super-Resolution》","content":"在这项工作中，我们试图量化以及可视化静态注意力机制，并说明了并非所有的注意力模块都是有益的。然后我们提出了attentioninattentionnetwork(A2N)用于高精确图像SR。具体来说，我们的A2N包括一个非注意力分支和一个注意力分支。我们提出了基于输入特征的注意力退出（Attentiondropout）模块，为这两个分支生成动态注意力权重，抑制不必要的注意力的调整。这使得注意力模块可以专注于对模型有益的部分，只需很小的额外开销就大大提高了注意力网络的能力。实验表明，与SOTA轻量级网络相比，我们的模型可以达到更好的性能与效率的权衡。局部归因图的实验也证明了A2结构中的注意力机制可以在更大的范围提取特征。MyNote添加注意力会增强哪些特征？对于CNN来说，网络低层的注意力倾向于增强特征的低频信息，而高层的注意于则倾向于增强特征的高频信息。（关于图像的高低频信息的概念可以看这篇博客，另外这些信息可以通过高通滤波器或低通滤波器获取）注意力层都是有用的吗？并不是所有添加的注意力层都能提高网络的性能，有些位置的注意力层是低效的、冗余的。通常网络较深的部分的注意力会比较有效。因为我们不可能人工去测试每一个位置的注意力是否是有效的，然后再确定网络的结构，所以文章提出了attentiondropout模块，用来动态地取消一些无效的注意力。通过该模块来产生两个和为1的权值（限制两个权值的和为1可以简化学习）来控制注意力分支和非注意力分支，其结构如下所示：关于A2B结构的一点小疑惑：设计的初衷应该是去除不必要的注意力以此减少参数提高模型计算效率，但是使用A2B结构虽然可以取消无效的注意力，但是并不能取消这部分的计算，相反还引入了额外的参数与计算量。因为有些注意力虽然是冗余的，但是基本不会对模型有负面的影响，所以直接保留所有的注意力是不会导致模型的性能下降的，而且其参数量是比引入A2B结构小的。不过引入A2B结构的额外开销很小且带来了模型性能的提升，并且即使缩减参数量到相同的水平，拥有A2B结构的模型依旧能获得更好的性能，这说明这是一个非常优秀的结构。但是该结构并不能直接去除不必要的注意力以此减少参数提高模型计算效率，而是要通过减少通道数或去除一些别的部分来减少参数量，这样的话该方法是不是适用于所有注意力层就不得而知了。实验中表明该结构在没有池化和下采样层的模型中使用时表现较好。（一些奇奇怪怪的想法，可能是我太菜了QAQ）最近看到好多轻量级的超分辨率网络都是使用nearest-neighbor插值上采样，个人感觉应该是为了提高计算效率，反正插值后的结果会有神经网络模块进行调整，所以最终结果应该和使用bicubic插值上采样差不多。之后有时间可以研究一下。papercodeContributions我们量化了神经网络中不同阶段的注意力层的有效性，并提出了一种有效的注意力层的修剪策略。我们提出了attentioninattentionblock(A2B)，它动态地为内部分支生成加权和为1的注意力。其中一个分支是注意力力分支，因此我们的A2B为注意力操作添加注意力。我们提出了基于A2B的A2N，与使用类似结构的baseline网络相比，其性能更优，参数开销更小。AttentioninAttentionNetwork","link":"https://ciki000.github.io/post/lun-wen-yue-du-lesslessattention-in-attention-network-for-image-super-resolutiongreatergreater/"},{"title":"论文阅读：《HINet: Half Instance Normalization Network for Image Restoration》","content":"本文探讨了InstanceNormalization在low-level视觉任务中的作用。具体来说，我们提出了一种新的块：HalfInstanceNormalizationBlock（HIN块），用于提高图像复原网络的性能。基于HIN块，我们设计了一个简单而强大的多级网络HINet，它由两个子网组成。在HIN块的帮助下，HINet在各种图像复原任务上超过了SOTA。MyNoteBatchNormalization（BN）不适用于low-level视觉任务。因为图像复原任务通常在训练网络时使用较小的patch和batch，这会导致BN的统计量不稳定，并且图像复原任务中每个像素的信息都比较重要，这导致其对scale比较敏感，使用BN可能会导致图片中特有的细节丢失（BN的详细介绍可以看看李宏毅老师的视频，感觉讲的蛮形象的，让我对BN的理解更透彻了一些，之后要是有时间把李宏毅老师深度学习的系列视频都看一遍）。InstanceNormalization（IN）就是对单张图片的各个通道进行归一化，保证了每张图片实例之间的独立，并且可以在训练和推理时保持相同的归一化过程。因此IN比较适用于结果主要依赖于单个图像实例的生成任务。HIN块的结构如下图所示。使用HIN块可以提升编码器的性能，但不能提升解码器的性能。作者并没有具体说明为什么要这样设计，感觉就是把IN集成到了一个块中并进行了一些优化。最近好像看到很多这种多分支的结构，都是一个分支进行一些复杂的计算另一个分支传递原始特征最后合并在一起，不知道HIN块如果使用类似attention的带权合并是否能进一步优化性能。papercodeContributions集成InstanceNormalization(IN)到基础块并提出了HIN块。这是首个在图像复原任务中直接采用归一化技术而达到SOTA性能的模型。基于HIN块，设计了一种多阶段网络结构HINet用于图像复原任务。该方法以更少的计算量和推理时间达到了SOTA性能。通过大量实验证明了提出的HIN块和HINet的有效性。受益于HIN块，HINet在NTIRE2021ImageDeblurringChallenge-Track2中获得了第一名。HINNet","link":"https://ciki000.github.io/post/lun-wen-yue-du-lesslesshinet-half-instance-normalization-network-for-image-restorationgreatergreater/"}]}